hydra:
  run:
    dir: tools/cli/outputs/finetune/${hydra:runtime.choices.model}/${hydra:runtime.choices.data}/${run_name}
defaults:
  - model: moirai_1.1_R_large
  - data: train_dataset
  - val_data: val_dataset  
  - _self_

run_name: finetuning_scheduler_run
seed: 20
tf32: true
compile: false  
ckpt_path: null

finetuning_scheduler:
  gen_ft_sched_only: True
  ft_schedule: /home/saltysoup/energy_price_forecasting/tools/cli/finetune_scheduler_schedules/moirai_1.1_R_small/full_finetune_schedule.yaml
  max_depth: -1
  base_max_lr: 1e-04
  restore_best: true
  epoch_transitions_only: false
  reinit_optim_cfg: null
  reinit_lr_cfg: null
  allow_untested: false
  apply_lambdas_new_pgs: false
  logging_level: 20
  enforce_phase0_params: true
  frozen_bn_track_running_stats: true

trainer:
  _target_: lightning.Trainer
  accelerator: auto
  strategy: auto
  #go to devices: [0] # only use the good GPU 
  num_nodes: 1
  precision: 32
  logger:
      _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ${hydra:runtime.output_dir}
      name: logs
  callbacks:
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch
    - _target_: finetuning_scheduler.FinetuningScheduler
      ft_schedule: ${finetuning_scheduler.ft_schedule}
      max_depth: ${finetuning_scheduler.max_depth}
      base_max_lr: ${finetuning_scheduler.base_max_lr}
      restore_best: ${finetuning_scheduler.restore_best}
      gen_ft_sched_only: ${finetuning_scheduler.gen_ft_sched_only}
      epoch_transitions_only: ${finetuning_scheduler.epoch_transitions_only}
      reinit_optim_cfg: ${finetuning_scheduler.reinit_optim_cfg}
      reinit_lr_cfg: ${finetuning_scheduler.reinit_lr_cfg}
      allow_untested: ${finetuning_scheduler.allow_untested}
      apply_lambdas_new_pgs: ${finetuning_scheduler.apply_lambdas_new_pgs}
      logging_level: ${finetuning_scheduler.logging_level}
      enforce_phase0_params: ${finetuning_scheduler.enforce_phase0_params}
      frozen_bn_track_running_stats: ${finetuning_scheduler.frozen_bn_track_running_stats}
    - _target_: finetuning_scheduler.FTSEarlyStopping
      monitor: val/PackedNLLLoss
      min_delta: 0.0
      patience: 10
      mode: min
      strict: false
      verbose: true
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ${hydra:runtime.output_dir}/checkpoints
      monitor: val/PackedNLLLoss
      save_weights_only: true
      mode: min
      save_top_k: 1
      every_n_epochs: 1
      filename: "checkpoint_best"
      
  max_epochs: 100
  enable_progress_bar: true
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm

train_dataloader:
  _target_: uni2ts.data.loader.DataLoader
  batch_size: 32 
  batch_size_factor: 2.0
  cycle: true
  num_batches_per_epoch: 100
  shuffle: true
  num_workers: 0 
  collate_fn:
    _target_: uni2ts.data.loader.PackCollate
    max_length: ${model.module_kwargs.max_seq_len}
    seq_fields: ${cls_getattr:${model._target_},seq_fields}
    pad_func_map: ${cls_getattr:${model._target_},pad_func_map}
  pin_memory: true
  drop_last: false
  fill_last: false
  worker_init_fn: null
  prefetch_factor: 2
  persistent_workers: true

val_dataloader:
  _target_: uni2ts.data.loader.DataLoader
  batch_size: 32 
  batch_size_factor: 2.0
  cycle: false
  num_batches_per_epoch: null
  shuffle: false
  num_workers: 0
  collate_fn:
    _target_: uni2ts.data.loader.PackCollate
    max_length: ${model.module_kwargs.max_seq_len}
    seq_fields: ${cls_getattr:${model._target_},seq_fields}
    pad_func_map: ${cls_getattr:${model._target_},pad_func_map}
  pin_memory: false
  drop_last: false
  fill_last: true
  worker_init_fn: null
  prefetch_factor: 2
  persistent_workers: true 